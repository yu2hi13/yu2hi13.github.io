<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(123, 77, 77, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  .disabled-link {
    color: #999999;
    cursor: default;
    text-decoration: none;
  }
  </style>
  <link rel="shortcut icon" href="images/apple-touch-ri-logo-white-120x120.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Yuzhi Huang</title>
  <meta name="Yuzhi Huang's Homepage" http-equiv="Content-Type" content="Yuzhi Huang's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Yuzhi Huang 「黄誉之」</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/profile_fig/yuzhi.jpg"><img src="images/profile_fig/yuzhi.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a >CV</a> |
    <a href="mailto:yzhuang13@stu.xmu.edu.cn">Email</a> |
    <a href="https://github.com/yu2hi13">Github</a> |
    <br/>
    | <a href="https://scholar.google.com.hk/citations?user=XgWSoXkAAAAJ&hl=zh-CN">Google Scholar</a> |
    <a href="https://huggingface.co/yu2hi13">HuggingFace</a> |
    <br/>
    | <a href="https://www.linkedin.com/in/huangyuzhi/">LinkedIn</a> |
    <!-- <a href="https://www.instagram.com/huangyuzhi/">Instagram</a> | -->
    <a href="https://www.xiaohongshu.com/user/profile/605b47df000000000100b0a5">RedNote</a> |    
    </p>
    

    <!-- <p align="center" style="margin-top:-8px;">
      <iframe id="twitter-widget-0" 
              scrolling="no" 
              frameborder="0" 
              allowtransparency="true" 
              allowfullscreen="true" 
              class="twitter-follow-button twitter-follow-button-rendered" 
              style="position: static; visibility: visible; width: 156px; height: 20px;" 
              title="Twitter Follow Button" 
              src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=huangyuzhi&amp;show_count=false&amp;show_screen_name=true&amp;size=m" 
              data-screen-name="huangyuzhi">
      </iframe>
      <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
  </p> -->


    </td>
    <td width="70%" valign="top" align="justify">
      <p>I am a second-year master student in the School of Informatics at <a href="https://www.xmu.edu.cn/">Xiamen University</a>, advised by Prof. <a href="https://scholar.google.com/citations?user=k5hVBfMAAAAJ&hl=zh-CN">Xinghao Ding</a> in the SmartDSP group. I am also collaborating with Dr. <a href="https://www.ee.cuhk.edu.hk/~yxyuan/index.htm">Chenxin Li</a> from the <a href="https://vita-group.github.io/">AIM</a> group at the Chinese University of Hong Kong.
        <br>
      </p>
      
      <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; My current research primarily covers the following topics:
        <ul>
          <li><strong style="color: #27AE60;">2D Visual Perception/Understanding</strong>: 2D Object Detection/Segmentation</li>
          <li><strong style="color: #e18a3b;">MLLM Adaptation</strong>: Task-Specific Downstream Fine-Tuning</li>
          <li><strong style="color: #2092E4;">4D Content Analysis</strong>: 4D Reconstruction, 4D Perception</li>
          <li><strong style="color: #9A2036;">AI Agents (ongoing)</strong>: Planning and Decision-Making, Reinforcement learning</li>
        </ul>
      </p>

      <p style="color: #e60012;">I am currently looking for <strong>26 fall PhD positions</strong> and actively seeking collaborations! If you are interested in working together or have potential PhD opportunities, please feel free to reach out to me.</p>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>WeChat</strong>: Swaggyzz-13 &nbsp;&nbsp; <strong>Email</strong>: yzhuang13@stu.xmu.edu.cn        
      </p>
     
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Recent News</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
      <ul> 
        <li>[2025.02]&ensp; One paper (Track Any Anomalous Object) accepted at <strong>CVPR 2025</strong>.</li>
        <li>[2024.09]&ensp; One paper (Flaws can be Applause) accepted to <strong>NeurIPS 2024</strong>.</li>
        <li>[2024.07]&ensp; One paper (P^2SAM) accepted to <strong>ACM MM 2024</strong>.</li>
      </ul>
    </td>
  </tr>
  
</table>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading> &nbsp;&nbsp; ( * denotes equal contribution )</td></tr>
</table>
 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">









  <tr>
    <td width="40%" valign="top" align="center"><a href="https://dynamic-verse.github.io/">
    <video playsinline autoplay loop muted src="images/dynamic_verse/DynamicVerse-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://dynamic-verse.github.io/" id="DynamicVerse">
      <heading>DynamicVerse: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds</heading></a><br>
      Kairun Wen*, <u>Yuzhi Huang*</u>, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, Jian Zhang, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan<br>
      NeurIPS 2025 <br>
      </p>

      <div class="paper" id="DynamicVerse">
      <a href="https://dynamic-verse.github.io/">Project</a> |
      <a>Paper</a> |
      <a href="javascript:toggleblock('DynamicVerse_abs')">Abstract</a> 


      <p align="justify"> <i id="DynamicVerse_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human‑like capabilities.
        However, existing datasets are often derived from limited simulators or utilize traditional Structure-from-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet.
        To bridge these gaps, we introduce <strong><span style="color: rgb(32, 146, 228);">DynamicVerse</span></strong>, a physical‑scale, multimodal 4D modeling framework for real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. 
        By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. 
        DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos.
        Experimental evaluations on three benchmark tasks---video depth estimation, camera pose estimation, and camera intrinsics estimation---validate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.
      </i></p>

<!-- <!-- <pre xml:space="preserve">
  @misc{lin2024unsupervisedlowlightimageenhancement,
    title={Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors}, 
    author={Yunlong Lin and Zhenqi Fu and Yuzhi Huang and Tian Ye and Sixiang Chen and Ge Meng and Yingying Wang and Yue Huang and Xiaotong Tu and Xinghao Ding},
    year={2024},
    eprint={2409.18899},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2409.18899}, 
  }
</pre> -->
      </div>
    </td>
  </tr>






  <tr>
    <td width="40%" valign="top" align="center"><a href="https://tao-25.github.io/">
    <img src="images/TAO/TAO.png" alt="TAO project" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://tao-25.github.io/" id="Tao">
      <heading>Track Any Anomalous Object: A Granular Video Anomaly Detection Pipeline</heading></a><br>
      <u>Yuzhi Huang*</u>, Chenxin Li*, Haitao Zhang, Zixu Lin, Yunlong Lin, Hengyu Liu, Wuyang Li, Xinyu Liu, Jiechao Gao, Yue Huang, Xinghao Ding, Yixuan Yuan <br>
      CVPR 2025 <br>
      </p>

      <div class="paper" id="TAO">
      <a href="https://tao-25.github.io/">Project</a> |
      <a href="papers/CVPR2025__TAO.pdf">Paper</a> |
      <a href="javascript:toggleblock('TAO_abs')">Abstract</a> |
      <a>Code</a>
      

      <p align="justify"> <i id="TAO_abs">
        <!-- &nbsp;&nbsp;&nbsp;&nbsp;Vision-centric perception systems struggle with unpredictable and coupled weather degradations in the wild. Current solutions are often limited, as they either depend on specific degradation priors or suffer from significant domain gaps. To enable robust and operation in real-world conditions, we propose <strong><span style="color: rgb(32, 146, 228);">JarvisIR</span></strong>, a VLM-powered agent that leverages the VLM as a controller to manage multiple expert restoration models. To further enhance system robustness, reduce hallucinations, and improve generalizability in real-world adverse weather, JarvisIR employs a novel two-stage framework consisting of supervised fine-tuning and human feedback alignment. Specifically, to address the lack of paired data in real-world scenarios, the human feedback alignment enables the VLM to be fine-tuned effectively on large-scale real-world data in an unsupervised manner. To support the training and evaluation of JarvisIR, we introduce CleanBench, a comprehensive dataset consisting of high-quality and large-scale instruction-responses pairs, including 150K synthetic entries and 80K real entries. Extensive experiments demonstrate that JarvisIR exhibits superior decision-making and restoration capabilities. Compared with existing methods, it achieves a 50% improvement in the average of all perception metrics on CleanBench-Real. -->
        &nbsp;&nbsp;&nbsp;&nbsp;Video anomaly detection (VAD) is crucial in scenarios such as surveillance and autonomous driving, where timely detection of unexpected activities is essential. Albeit existing methods have primarily focused on detecting anomalous objects in videos—either by identifying anomalous frames or objects—they often neglect finer-grained analysis, such as anomalous pixels, which limits their ability to capture a broader range of anomalies. To address this challenge, we propose an innovative VAD framework called Track Any Anomalous Object (<strong><span style="color: rgb(32, 146, 228);">TAO</span></strong>), which introduces a Granular Video Anomaly Detection Framework that, for the first time, integrates the detection of multiple fine-grained anomalous objects into a unified framework. Unlike methods that assign anomaly scores to every pixel at each moment, our approach transforms the problem into pixel-level tracking of anomalous objects. By linking anomaly scores to subsequent tasks such as image segmentation and video tracking, our method eliminates the need for threshold selection and achieves more precise anomaly localization, even in long and challenging video sequences. Experiments on extensive datasets demonstrate that <strong><span style="color: rgb(32, 146, 228);">TAO</span></strong>achieves state-of-the-art performance, setting a new progress for VAD by providing a practical, granular, and holistic solution.
      </i></p>

<!-- <pre xml:space="preserve">
  @inproceedings{jarvisir2025,
    title={JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration},
    author={Lin, Yunlong and Lin, Zixu and Chen, Haoyu and Pan, Panwang and Li, Chenxin and Chen, Sixiang and Huang, Yuzhi and Jin, Yeying and Li, Wenbo and Ding, Xinghao},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2025}
  }
</pre> -->
      </div>
    </td>
  </tr>












  <tr>
    <td width="40%" valign="top" align="center"><a href="https://a-sa-m.github.io/">
      <img src="images/ASAM/asam.png" alt="ASAM project" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://a-sa-m.github.io/" id="ASAM">
      <heading>Flaws can be Applause: Unleashing Potential of Segmenting Ambiguous Objects in SAM</heading></a><br>
      Chenxin Li*, <u>Yuzhi Huang*</u>, Wuyang Li, Hengyu Liu, Xinyu Liu, Qing Xu, Zhen Chen, Yue Huang, Yixuan Yuan <br>
      NeurIPS 2024 <br>
      </p> 
      <div class="paper" id="ASAM">
      <a href="https://a-sa-m.github.io/">Project</a> |
      <a href="https://openreview.net/pdf?id=vJSNsSFO95">Paper</a> |
      <a href="javascript:toggleblock('javisir_abs')">Abstract</a> |
      <a href="https://github.com/chenxinli001/a2sam">Code</a> 

      <p align="justify"> <i id="ASAM_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;As the vision foundation models like the Segment Anything Model (SAM) demonstrate potent universality, they also present challenges in giving ambiguous and uncertain predictions. Significant variations in the model output and granularity can occur with simply subtle changes in the prompt, contradicting the consensus requirement for the robustness of a model. While some established works have been dedicated to stabilizing and fortifying the prediction of SAM, this paper takes a unique path to explore how this flaw can be inverted into an advantage when modeling inherently ambiguous data distributions. We introduce an optimization framework based on a conditional variational autoencoder, which jointly models the prompt and the granularity of the object with a latent probability distribution. This approach enables the model to adaptively perceive and represent the real ambiguous label distribution, taming SAM to produce a series of diverse, convincing, and reasonable segmentation outputs controllably. Extensive experiments on several practical deployment scenarios involving ambiguity demonstrates the exceptional performance of our framework.
      </i></p>
<!-- <pre xml:space="preserve">
  @misc{fan2023lightgaussian, 
    title={LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS}, 
    author={Zhiwen Fan and Kevin Wang and Yuzhi Huang and Zehao Zhu and Dejia Xu and Zhangyang Wang}, 
    year={2023},
    eprint={2311.17245},
    archivePrefix={arXiv},
    primaryClass={cs.CV} 
  }
</pre> -->
      </div>
    </td>
  </tr>

  

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://p2-sam.github.io/">
      <img src="images/P2SAM/p2sam.png" alt="P2SAM project" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://p2-sam.github.io/" id="P2SAM">
      <heading>P²SAM: Probabilistically Prompted SAMs Are Efficient Segmentator for Ambiguous Medical Images</heading></a><br>
      <u>Yuzhi Huang*</u>, Chenxin Li*, Zixu Lin, Hengyu Liu, Haote Xu, Yifan Liu, Yue Huang, Xinghao Ding, Yixuan Yuan <br>
      ACM MM 2024 <br>
      </p>

      <div class="paper" id="p2sam">
      <a href="https://p2-sam.github.io/">Project</a> |
      <a href="papers/ACM_MM2024__P2SAM.pdf">Paper</a> |
      <a href="javascript:toggleblock('p2sam_abs')">Abstract</a> |
      <a href="https://github.com/yu2hi13/P2SAM">Code</a> 

      <p align="justify"> <i id="p2sam_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;The ability to generate an array of plausible outputs for a single input has profound implications for dealing with inherent ambiguity in visual scenarios. This is evident in scenarios where diverse semantic segmentation annotations for a single medical image are provided by various experts. Existing methods hinge on probabilistic modelling of representations to depict this ambiguity and rely on extensive multi-output annotated data to learn this probabilistic space.

        However, these methods often falter when only a limited amount of ambiguously labelled data is available, which is a common occurrence in real-world applications. To surmount these challenges, we propose a novel framework, termed as (<strong><span style="color: rgb(32, 146, 228);">P²SAM</span></strong>), that leverages the prior knowledge of the Segment Anything Model (SAM) during the segmentation of ambiguous objects. Specifically, we delve into an inherent drawback of SAM in deterministic segmentation, i.e., the sensitivity of output to prompts, and ingeniously transform this into an advantage for ambiguous segmentation tasks by introducing a prior probabilistic space for prompts.
        
        Experimental results demonstrate that our strategy significantly enhances the precision and diversity of medical segmentation through the utilization of a small number of ambiguously annotated samples by doctors. Rigorous benchmarking experiments against state-of-the-art methods indicate that our method achieves superior segmentation precision and diversified outputs with fewer training data (using simply 5.5% samples, +12% Dmax).
        
        The (<strong><span style="color: rgb(32, 146, 228);">P²SAM</span></strong>) signifies a substantial step towards the practical deployment of probabilistic models in real-world scenarios with limited data.
      </i></p>

<!-- <pre xml:space="preserve">
  @misc{fan2024largespatialmodelendtoend,
    title={Large Spatial Model: End-to-end Unposed Images to Semantic 3D}, 
    author={Zhiwen Fan and Jian Zhang and Wenyan Cong and Peihao Wang and Renjie Li and Yuzhi Huang and Shijie Zhou and Achuta Kadambi and Zhangyang Wang and Danfei Xu and Boris Ivanovic and Marco Pavone and Yue Wang},
    year={2024},
    eprint={2410.18956},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2410.18956}, 
  }
</pre> -->
      </div>
    </td>
  </tr>












<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>Honors & Awards</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      Huang Xilie University-Level Scholarship, 2025 
      <br>
      University Level Outstanding Student Award, 2024
      <br>
      Meritorious Winner in the Mathematical Contest in Modeling (MCM), USA, 2024
      </p>
    </td>
  </tr>
</table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>Reviewer Services</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      International Conference on Machine Learning <b>(ICML)</b>, 2025
      <br>
      International Conference on Learning Representations <b>(ICLR)</b>, 2025
      <br>
      Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2025, 2024
      </p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
      <tr>
          <td style="padding:0px">
              <br><br>
              <div>
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=bqvXDgMuKZCrCZYT2Wh6KGPxri2jjl9cxis6tO7DjI4&cl=ffffff&w=a"></script>
              </div>
          </td>
      </tr>
  </tbody>
</table>







<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="https://tairanhe.com/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acm_turc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('aog_mcts_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pragmatics_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('collab_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lsm_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lightgaussian_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('instantsplat_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('sam3r_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('dplut_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('javisir_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('agile-but-safe_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('safedpa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acs_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('saferl_survey_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('patchail_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('sisos_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('uaissa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('autocost_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('a2ls_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('issa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ebil_abs');
</script>
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('maniploco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('parkour_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mobile_aloha_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('DynamicVerse_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('TAO_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('p2sam_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ASAM_abs');
</script>
</body>

</html>
